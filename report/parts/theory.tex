%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% coding: utf-8
%%% End:
% !TEX TS-program = pdflatexmk
% !TEX encoding = UTF-8 Unicode
% !TEX root = ../main.tex

\section{Computer Graphics}

This section provides an overview of computer graphics, including its history, key concepts and applications.

Since the early days of computing, researchers have explored ways to process visual information using computers. While the field also encompasses aspects such as image processing or two-dimensional graphics, the main focus of this thesis is on three-dimensional computer graphics. While this includes topics such as animation, geometry processing, the main topics covered are geometry representation, rendering and shading.

\subsection{GPU}

The GPU is a specialized processor that is well-suited for workloads which require parallel processing. GPUs are widely used in computer graphics and other disciplines such as Machine Learning relying on parallel processing.

The first GPUs were developed in the 1990s and have since been integrated into most consumer hardware including smartphones, tablets, and laptops.

While CPU parallelization on consumer hardware is generally limited to a few cores, GPUs have thousands of compute units.

\subsection{Rendering Approaches}

In order to visualize 3d scenes using a computer, different rendering approaches have been developed. Two of the most common approaches are rasterization and ray tracing and will be described in more detail in the following sections.

\subsubsection{Rasterization}

Rasterization is a rendering technique which maps the geometry of a 3d scene onto a 2d plane. Historically, the technique has been widely adopted in real-time rendering due to its efficiency. However, there are inherent limitations in terms of realism.

One of the main limitations is the lack of support for global illumination. A related shortcoming is the difficulty to generate ambient occlusion. Certain techniques such as using pre-baked shadow, environment and light maps, screen space ambient occlusion (SSAO) \cite{bavoil2008ssao}, screen space directional occlusion (SSDO) \cite{ritschel2009ssdo} or ray traced ambient occlusion (RTAO) \cite{gautron2020rtao} have been developed to address these limitations.

These approaches induce complexity and can be computationally expensive. An alternative technique which resembles reality more closely could alleviate these issues: Ray Tracing.

\subsubsection{Ray Tracing}

Ray Tracing is a rendering technique which simulates light transport in a scene. The main idea is to cast rays from the camera into the scene and compute the color of the object the ray intersects with. By adding additional bounces of the light ray, the technique can simulate global illumination.

One of the earliest papers describing approaches to solve shadow casting was written as early 1968 by Appel \cite{appel1968shading}. The term global illumination was coined by Whitted in 1979 \cite{whitted2020OriginsOfGlobalIllumination}.

Due to the computational complexity of these algorithms, ray tracing was not widely adopted until the 1980s. Some of the first widely used ray tracing software was POV-Ray, which was released in 1991 \cite{POV_Ray_Documentation}. Blue Moon Rendering Tools (BMRT) \cite{bmrt}, a RenderMan compliant renderer, was released in the mid 90s and was one of the first ray tracing renderers to be used in the industry. In 2003 PRMan included ray tracing capabilities \cite{RenderMan_11_Release_Notes}.

The rendering equation has been defined in 1986 in a paper which also coined the term path tracing \cite{kajiya1986rendering}. Generally, path tracing can be seen as a Monte Carlo integration of the rendering equation which is defined as

\begin{equation}
  \label{eqn:rendering-equation}
  I(x, x') = g(x, x') [\epsilon(x, x') + \int_{S} p(x, x', x'')I(x', x'')dx'']
\end{equation}

Where $I(x, x')$ is the radiance from point $x$ to point $x'$. $x$ for example being the camera position and $x'$ the intersected object. $g(x, x')$ is a geometry term determining how much light is transmitted. This depends on the distance and possibly occlusions such as in the case of transparent surfaces. $\epsilon(x, x')$ is the emitted radiance, generally used for light sources. The integral term is taken over $S$ = $\cup S_i$ which is the union of all surfaces. $p(x, x', x'')$ is the bidirectional reflection function which describes how light is reflected at the surface. \cite{kajiya1986rendering}

Further research into light transport techniques such as bidirectional ligh transport or Metropolis Light Transport has been conducted in the 1990s \cite{veachMonteCarloLightTransport}.

\todo{This is only for showing citation style}

\subsection{Graphic APIs}
\subsubsection{OpenGL}

OpenGL is an API for rendering 3d graphics. After its introduction in 1992, it was widely adopted. Subsequently, the standard has been ported to other platforms and has been extended with new features.

To date, OpenGL is still widely used in the industry, but it has been replaced by more modern APIs in recent years.

\subsubsection{Vulkan, Metal, DirectX}

While OpenGL and derivatives such as OpenGL ES have been widely adopted, the introduction of a number of new APIs has changed the landscape. Some of the most notable APIs are:

\begin{itemize}
    \item{Vulkan} Developed by the Khronos Group, Vulkan is a low-level API which is supported on a wide range of platforms.
    \item{Metal} Developed by Apple, Metal is a low-level API which is supported on Apple platforms.
    \item{DirectX} Developed by Microsoft, DirectX is a collection of APIs which are supported on Windows platforms.
\end{itemize}

\subsubsection{WebGL}

WebGL is a graphics API based on OpenGL ES 2.0 for the web. It was initially released in 2011 and has since been adopted by all major browsers.

WebGL is designed to offer a rendering pipeline, but does not offer GPGPU capabilities. There have been efforts to extend with compute shaders, but efforts by the Khronos Group have been halted in favor of focusing on WebGPU instead.

In order to provide GPGPU capabilities, workarounds have been developed. The basic idea is to render the output of a fragment shader to a texture and interpreting the output as binary data instead of color information.
There are libraries such as GPU.js which provide GPGPU capabilities using WebGL. Tensorflow.js, a library for training and deploying Machine Learning models, use similar techniques in the WebGL backend.

Since its introduction, WebGL has been extended with new features. WebGL 2.0, which is based on OpenGL ES 3.0, was released in 2017. Of the major browsers, Safari was the last to support it out of the box in 2021. Since WebGL 2.0 no new major versions have been released but new features have been added.

\subsubsection{WebGPU}

WebGPU is a new web standard which is no longer based on OpenGL. One of the main capabilities is support for GPGPU by design. While all major browser vendors have announced intent to support WebGPU, to date only Chrome has shipped WebGPU for general use on desktop as well as mobile.
The standard is still in development and new features are being added.
Common 3d engines such as Babylon.js, Three.js, PlayCanvas and Unity have announced support for WebGPU.

\section{Ray Tracing}
\subsection{Monte Carlo Ray Tracing}
\subsection{Intersection Testing}

When casting a ray into the scene, the first step in a ray tracer is to determine where the ray intersects with the geometry of the scene.

A brute force approach could be to determine the intersection distance for each object to the ray and picking the one with the lowest distance. Given the fact, that common objects consist of thousands of triangles and ray tracing many rays, this approach is not efficient.

In order to accelerate the intersection testing, various approaches have been developed.

\fgls{BVH}{\e{Bounding Volume Hierarchy}, common tree-based acceleration structure}

\subsection{Importance Sampling}

\section{Physically Based Rendering}

Defining the geometry is one part of the equation. Another major part is defining materials. Materials define how a surface interacts with light. The core idea of physically based rendering (PBR) is to simulate the interaction of light using physical models. Instead of fine-tuning parameters for a specific look and feel and having to adjust based on desired lighting situations, PBR aims to define a material which behaves consistently in different lighting situations.

\subsection{BxDF}
\subsection{Microfacet Theory}

While the macroscopic appearance of a material is visible to the human eye, the microscopic structure of the material is crucial in determining how light interacts with the material. Microfacet theory is a model which describes this microscopic structure. The distribution of the normals of the microfacet is described by a distribution function.

\subsection{Exchange Formats}

In order to exchange 3d scenes between a multitude of applications, various standardized formats have been developed. These formats are optimized for different use cases depending on the requirements of the application.

\subsubsection{General Purpose Formats}

One of the most widely used formats is Wavefront OBJ which was established in the 1980s. The format is text-based and has basic support for materials and textures. However, it lacks support for more advanced features such animations. Additionally, due to its encoding, it is not well-suited for delivery over the web compared to more modern alternatives.

Formats such as the proprietary FBX by Autodesk established in 2006 can address the shortcomings in terms of advanced features. The format is supported by a wide range of applications.

\subsubsection{Interoperability Formats}

Other formats such as COLLADA have been developed to improve transporting 3d scenes between different applications. It is XML-based and has been established in 2004.

Since, other formats such as USD and has been open-sourced by Pixar in 2016.

\subsubsection{Runtime Formats}

For usage in end-user applications, such as for this thesis, the glTF format is well-suited. It has been established in 2015 and is designed to be efficient for transmission and loading of 3d scenes.

\subsection{Material Description}

A variety of standardized material description models have been developed over the years.

\subsubsection{MaterialX}

MaterialX is an open standard which was originally developed by Lucasfilm in 2012. The standard has since been adopted by the Academy Software Foundation as an open standard.

While not strictly limited to PBR, the standard provides a wide range of features to describe physically based materials \cite{Harrysson2019}.

\subsubsection{OpenPBR}

OpenPBR is hosted by the Academy Software Foundation as an open standard. It differs from MaterialX in that it is providing an über-shader approach instead of a node-based approach. It serves as a combination of Autodesk Standard Surface and Adobe Standard Material and is being worked on by both companies. The über-shader approach is defined by having a fix set of inputs which can be adjusted, but it does not allow for custom node graphs as MaterialX does.
